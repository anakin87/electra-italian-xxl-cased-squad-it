{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# electra-italian-xxl-cased-squad-it: training\n","\n","Electra model for (Extractive) Question Answering on Italian texts\n","This model has been fine-tuned on squad_it dataset, starting from the pre-trained model dbmdz/electra-base-italian-xxl-cased-discriminator.\n","\n","Training notebook inspired by https://huggingface.co/course/chapter7/7 and https://github.com/luigisaetta/nlp-qa-italian/blob/main/train_squad_it_final1.ipynb"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:47:45.655474Z","iopub.status.busy":"2023-01-31T07:47:45.655068Z","iopub.status.idle":"2023-01-31T07:47:45.666849Z","shell.execute_reply":"2023-01-31T07:47:45.665270Z","shell.execute_reply.started":"2023-01-31T07:47:45.655447Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","from datasets import load_metric\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","from transformers import pipeline\n","\n","from tqdm.auto import tqdm\n","\n","# to compute metrics in evaluation phase\n","import numpy as np\n","import collections\n","\n","# for the custom training loop\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","from transformers import default_data_collator\n","from accelerate import Accelerator\n","from transformers import get_scheduler\n","\n","from accelerate import Accelerator"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:47:45.681484Z","iopub.status.busy":"2023-01-31T07:47:45.680810Z","iopub.status.idle":"2023-01-31T07:47:45.700618Z","shell.execute_reply":"2023-01-31T07:47:45.697734Z","shell.execute_reply.started":"2023-01-31T07:47:45.681447Z"},"trusted":true},"outputs":[],"source":["# try to ensure reproducibility\n","\n","import torch\n","torch.manual_seed(0)\n","import random\n","random.seed(0)\n","import numpy as np\n","np.random.seed(0)\n","g = torch.Generator()\n","g.manual_seed(0)\n","g2 = torch.Generator()\n","g2.manual_seed(0)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load squad_it dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:47:45.711287Z","iopub.status.busy":"2023-01-31T07:47:45.709129Z","iopub.status.idle":"2023-01-31T07:48:00.188670Z","shell.execute_reply":"2023-01-31T07:48:00.187694Z","shell.execute_reply.started":"2023-01-31T07:47:45.711201Z"},"trusted":true},"outputs":[],"source":["raw_datasets = load_dataset(\"squad_it\")\n","metric = load_metric(\"squad\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:48:18.202349Z","iopub.status.busy":"2023-01-31T07:48:18.201792Z","iopub.status.idle":"2023-01-31T07:48:18.209914Z","shell.execute_reply":"2023-01-31T07:48:18.208838Z","shell.execute_reply.started":"2023-01-31T07:48:18.202306Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'context', 'question', 'answers'],\n","        num_rows: 54159\n","    })\n","    test: Dataset({\n","        features: ['id', 'context', 'question', 'answers'],\n","        num_rows: 7609\n","    })\n","})"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["raw_datasets"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset preprocessing"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:48:19.213854Z","iopub.status.busy":"2023-01-31T07:48:19.213125Z","iopub.status.idle":"2023-01-31T07:48:19.219662Z","shell.execute_reply":"2023-01-31T07:48:19.218404Z","shell.execute_reply.started":"2023-01-31T07:48:19.213809Z"},"trusted":true},"outputs":[],"source":["# here we define the base model, pre-trained on Italian language\n","MODEL_CHECKPOINT = \"dbmdz/electra-base-italian-xxl-cased-discriminator\"\n","\n","MAX_LENGTH = 384\n","STRIDE = 128\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:48:30.763742Z","iopub.status.busy":"2023-01-31T07:48:30.763218Z","iopub.status.idle":"2023-01-31T07:48:30.784757Z","shell.execute_reply":"2023-01-31T07:48:30.783634Z","shell.execute_reply.started":"2023-01-31T07:48:30.763702Z"},"trusted":true},"outputs":[],"source":["#\n","# functions for preprocessing the dataset\n","#\n","\n","def preprocess_training_examples(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=MAX_LENGTH,\n","        truncation=\"only_second\",\n","        stride=STRIDE,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n","    answers = examples[\"answers\"]\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, offset in enumerate(offset_mapping):\n","        sample_idx = sample_map[i]\n","        answer = answers[sample_idx]\n","        start_char = answer[\"answer_start\"][0]\n","        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","        # Find the start and end of the context\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        context_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        context_end = idx - 1\n","\n","        # If the answer is not fully inside the context, label is (0, 0)\n","        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # Otherwise it's the start and end token positions\n","            idx = context_start\n","            while idx <= context_end and offset[idx][0] <= start_char:\n","                idx += 1\n","            start_positions.append(idx - 1)\n","\n","            idx = context_end\n","            while idx >= context_start and offset[idx][1] >= end_char:\n","                idx -= 1\n","            end_positions.append(idx + 1)\n","\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","    return inputs\n","\n","\n","def preprocess_validation_examples(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=MAX_LENGTH,\n","        truncation=\"only_second\",\n","        stride=STRIDE,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n","    example_ids = []\n","\n","    for i in range(len(inputs[\"input_ids\"])):\n","        sample_idx = sample_map[i]\n","        example_ids.append(examples[\"id\"][sample_idx])\n","\n","        sequence_ids = inputs.sequence_ids(i)\n","        offset = inputs[\"offset_mapping\"][i]\n","        inputs[\"offset_mapping\"][i] = [\n","            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n","        ]\n","\n","    inputs[\"example_id\"] = example_ids\n","    return inputs"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:48:36.154752Z","iopub.status.busy":"2023-01-31T07:48:36.154019Z","iopub.status.idle":"2023-01-31T07:49:15.515949Z","shell.execute_reply":"2023-01-31T07:49:15.514914Z","shell.execute_reply.started":"2023-01-31T07:48:36.154716Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b58bbdda0480490199559851e780d5aa","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/55 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["# rec. in raw dataset 54159, in final train set: 55137\n"]}],"source":["train_dataset = raw_datasets[\"train\"].map(\n","    preprocess_training_examples,\n","    batched=True,\n","    remove_columns=raw_datasets[\"train\"].column_names,\n",")\n","\n","print(f'# rec. in raw dataset {len(raw_datasets[\"train\"])}, in final train set: {len(train_dataset)}')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:49:15.519099Z","iopub.status.busy":"2023-01-31T07:49:15.518456Z","iopub.status.idle":"2023-01-31T07:50:00.181171Z","shell.execute_reply":"2023-01-31T07:50:00.180039Z","shell.execute_reply.started":"2023-01-31T07:49:15.519056Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f9ef1f7612546378bebb310131decad","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/8 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["# rec. in raw dataset 7609, in final validation set: 7853\n"]}],"source":["validation_dataset = raw_datasets[\"test\"].map(\n","    preprocess_validation_examples,\n","    batched=True,\n","    remove_columns=raw_datasets[\"test\"].column_names,\n",")\n","\n","print(f'# rec. in raw dataset {len(raw_datasets[\"test\"])}, in final validation set: {len(validation_dataset)}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Evaluation functions"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:48:31.883576Z","iopub.status.busy":"2023-01-31T07:48:31.883066Z","iopub.status.idle":"2023-01-31T07:48:31.905145Z","shell.execute_reply":"2023-01-31T07:48:31.903727Z","shell.execute_reply.started":"2023-01-31T07:48:31.883529Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(start_logits, end_logits, features, examples):\n","    n_best = 20\n","    max_answer_length = 30\n","    \n","    example_to_features = collections.defaultdict(list)\n","    for idx, feature in enumerate(features):\n","        example_to_features[feature[\"example_id\"]].append(idx)\n","\n","    predicted_answers = []\n","    for i,example in enumerate(tqdm(examples)):\n","        if i > len(start_logits):\n","            break\n","        example_id = example[\"id\"]\n","        context = example[\"context\"]\n","        answers = []\n","\n","        # Loop through all features associated with that example\n","        for feature_index in example_to_features[example_id]:\n","            start_logit = start_logits[feature_index]\n","            end_logit = end_logits[feature_index]\n","            offsets = features[feature_index][\"offset_mapping\"]\n","\n","            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n","            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    # Skip answers that are not fully in the context\n","                    if offsets[start_index] is None or offsets[end_index] is None:\n","                        continue\n","                    # Skip answers with a length that is either < 0 or > max_answer_length\n","                    if (\n","                        end_index < start_index\n","                        or end_index - start_index + 1 > max_answer_length\n","                    ):\n","                        continue\n","                    try:\n","                        answer = {\n","                            \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n","                            \"logit_score\": start_logit[start_index] + end_logit[end_index],\n","                        }\n","                        answers.append(answer)\n","                    except:\n","                        pass\n","\n","        # Select the answer with the best score\n","        if len(answers) > 0:\n","            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n","            predicted_answers.append(\n","                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n","            )\n","        else:\n","            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n","\n","    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n","    \n","    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:48:34.854084Z","iopub.status.busy":"2023-01-31T07:48:34.853578Z","iopub.status.idle":"2023-01-31T07:48:34.865092Z","shell.execute_reply":"2023-01-31T07:48:34.862695Z","shell.execute_reply.started":"2023-01-31T07:48:34.854047Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(trainer, val_dataset, raw_val_dataset):\n","    predictions, _, _ = trainer.predict(val_dataset)\n","    start_logits, end_logits = predictions\n","\n","    eval_metrics = compute_metrics(start_logits, end_logits, val_dataset, raw_val_dataset)\n","\n","    print()\n","    print(\"Evaluation metrics:\")\n","    print(f\"EM: {round(eval_metrics['exact_match'], 2)}, F1-score: {round(eval_metrics['f1'], 2)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Custom training loop"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:50:00.183981Z","iopub.status.busy":"2023-01-31T07:50:00.183352Z","iopub.status.idle":"2023-01-31T07:50:00.195014Z","shell.execute_reply":"2023-01-31T07:50:00.193928Z","shell.execute_reply.started":"2023-01-31T07:50:00.183926Z"},"trusted":true},"outputs":[],"source":["BATCH_SIZE = 8\n","LEARNING_RATE = 2e-5\n","EPOCHS = 2\n","\n","# directory where we save the model for each epoch\n","OUTPUT_DIR = \"electra-italian-xxl-cased-squad-it-v3\"\n","\n","train_dataset.set_format(\"torch\")\n","validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n","validation_set.set_format(\"torch\")\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    shuffle=True,\n","    collate_fn=default_data_collator,\n","    batch_size=BATCH_SIZE,\n","    generator=g\n",")\n","eval_dataloader = DataLoader(\n","    validation_set, \n","    collate_fn=default_data_collator, \n","    batch_size=BATCH_SIZE,\n","    generator=g2\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:50:00.197038Z","iopub.status.busy":"2023-01-31T07:50:00.196531Z","iopub.status.idle":"2023-01-31T07:50:23.185483Z","shell.execute_reply":"2023-01-31T07:50:23.184504Z","shell.execute_reply.started":"2023-01-31T07:50:00.197000Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea2ab9ea2d0b4cfcae4ba5b0dc84a3b7","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/419M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at dbmdz/electra-base-italian-xxl-cased-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n","- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at dbmdz/electra-base-italian-xxl-cased-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:50:33.607140Z","iopub.status.busy":"2023-01-31T07:50:33.606654Z","iopub.status.idle":"2023-01-31T07:50:40.076519Z","shell.execute_reply":"2023-01-31T07:50:40.075244Z","shell.execute_reply.started":"2023-01-31T07:50:33.607091Z"},"trusted":true},"outputs":[],"source":["optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","\n","# mixed mode (fp16)\n","accelerator = Accelerator(fp16=True)\n","\n","model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n","    model, optimizer, train_dataloader, eval_dataloader\n",")\n","\n","num_train_epochs = EPOCHS\n","num_update_steps_per_epoch = len(train_dataloader)\n","num_training_steps = num_train_epochs * num_update_steps_per_epoch\n","\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T07:51:44.087857Z","iopub.status.busy":"2023-01-31T07:51:44.087475Z","iopub.status.idle":"2023-01-31T08:50:40.157878Z","shell.execute_reply":"2023-01-31T08:50:40.156539Z","shell.execute_reply.started":"2023-01-31T07:51:44.087823Z"},"trusted":true},"outputs":[],"source":["#\n","# we need a custom training loop because the Trainer interface doesn't allow this\n","#\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","for epoch in range(EPOCHS):\n","    #\n","    # Training\n","    #\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        accelerator.backward(loss)\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)\n","\n","    #\n","    # Evaluation\n","    #\n","    model.eval()\n","    start_logits = []\n","    end_logits = []\n","    accelerator.print(\"Evaluation!\")\n","    for batch in tqdm(eval_dataloader):\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n","        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n","\n","    start_logits = np.concatenate(start_logits)\n","    end_logits = np.concatenate(end_logits)\n","    start_logits = start_logits[: len(validation_dataset)]\n","    end_logits = end_logits[: len(validation_dataset)]\n","\n","    metrics = compute_metrics(\n","        start_logits, end_logits, validation_dataset, raw_datasets[\"test\"]\n","    )\n","    print(f\"epoch {epoch}:\", metrics)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
